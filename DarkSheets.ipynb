{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ac9933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import LinAlgError\n",
    "import scipy\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from scipy.optimize.linesearch import scalar_search_wolfe2\n",
    "\n",
    "class LineSearchTool(object):\n",
    "    \"\"\"\n",
    "    Line search tool for adaptively tuning the step size of the algorithm.\n",
    "\n",
    "    method : String containing 'Wolfe', 'Armijo' or 'Constant'\n",
    "        Method of tuning step-size.\n",
    "        Must be be one of the following strings:\n",
    "            - 'Wolfe' -- enforce strong Wolfe conditions;\n",
    "            - 'Armijo\" -- adaptive Armijo rule;\n",
    "            - 'Constant' -- constant step size.\n",
    "    kwargs :\n",
    "        Additional parameters of line_search method:\n",
    "\n",
    "        If method == 'Wolfe':\n",
    "            c1, c2 : Constants for strong Wolfe conditions\n",
    "            alpha_0 : Starting point for the backtracking procedure\n",
    "                to be used in Armijo method in case of failure of Wolfe method.\n",
    "        If method == 'Armijo':\n",
    "            c1 : Constant for Armijo rule\n",
    "            alpha_0 : Starting point for the backtracking procedure.\n",
    "        If method == 'Constant':\n",
    "            c : The step size which is returned on every step.\n",
    "    \"\"\"\n",
    "    def __init__(self, method='Wolfe', **kwargs):\n",
    "        self._method = method\n",
    "        if self._method == 'Wolfe':\n",
    "            self.c1 = kwargs.get('c1', 1e-4)\n",
    "            self.c2 = kwargs.get('c2', 0.9)\n",
    "            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n",
    "        elif self._method == 'Armijo':\n",
    "            self.c1 = kwargs.get('c1', 1e-4)\n",
    "            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n",
    "        elif self._method == 'Constant':\n",
    "            self.c = kwargs.get('c', 1.0)\n",
    "        else:\n",
    "            raise ValueError('Unknown method {}'.format(method))\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, options):\n",
    "        if type(options) != dict:\n",
    "            raise TypeError('LineSearchTool initializer must be of type dict')\n",
    "        return cls(**options)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def line_search(self, oracle, x_k, d_k, previous_alpha=None):\n",
    "        \"\"\"\n",
    "        Finds the step size alpha for a given starting point x_k\n",
    "        and for a given search direction d_k that satisfies necessary\n",
    "        conditions for func_phi(alpha) = oracle.func(x_k + alpha * d_k).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        oracle : BaseSmoothOracle-descendant object\n",
    "            Oracle with .func_directional() and .grad_directional() methods implemented for computing\n",
    "            function values and its directional derivatives.\n",
    "        x_k : np.array\n",
    "            Starting point\n",
    "        d_k : np.array\n",
    "            Search direction\n",
    "        previous_alpha : float or None\n",
    "            Starting point to use instead of self.alpha_0 to keep the progress from\n",
    "             previous steps. If None, self.alpha_0, is used as a starting point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        alpha : float or None if failure\n",
    "            Chosen step size\n",
    "        \"\"\"\n",
    "        constant = lambda: self.c\n",
    "        \n",
    "        def armijo():\n",
    "            alpha = previous_alpha or self.alpha_0\n",
    "            while func_phi(alpha) > (phi0 + alpha * dphi0 * self.c1):\n",
    "                alpha /= 2\n",
    "            return alpha\n",
    "        \n",
    "        def wolfe():\n",
    "            alpha = scalar_search_wolfe2(func_phi, func_phi_der, phi0, None, dphi0, self.c1, self.c2)[0]\n",
    "            if alpha:\n",
    "                return alpha\n",
    "            return armijo()\n",
    "        \n",
    "        func_phi = lambda x: oracle.func_directional(x_k, d_k, x)\n",
    "        func_phi_der = lambda x: oracle.grad_directional(x_k, d_k, x)\n",
    "        phi0 = func_phi(0)\n",
    "        dphi0 = func_phi_der(0)\n",
    "        \n",
    "        methods_dict = {\n",
    "            'Wolfe': wolfe,\n",
    "            'Armijo': armijo,\n",
    "            'Constant': constant\n",
    "        }\n",
    "\n",
    "        return methods_dict[self._method]()\n",
    "\n",
    "def get_line_search_tool(line_search_options=None):\n",
    "    if line_search_options:\n",
    "        if type(line_search_options) is LineSearchTool:\n",
    "            return line_search_options\n",
    "        else:\n",
    "            return LineSearchTool.from_dict(line_search_options)\n",
    "    else:\n",
    "        return LineSearchTool()\n",
    "\n",
    "\n",
    "def gradient_descent(oracle, x_0, tolerance=1e-5, max_iter=10000,\n",
    "                     line_search_options=None, trace=False, display=False):\n",
    "    \"\"\"\n",
    "    Gradien descent optimization method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    oracle : BaseSmoothOracle-descendant object\n",
    "        Oracle with .func(), .grad() and .hess() methods implemented for computing\n",
    "        function value, its gradient and Hessian respectively.\n",
    "    x_0 : np.array\n",
    "        Starting point for optimization algorithm\n",
    "    tolerance : float\n",
    "        Epsilon value for stopping criterion.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations.\n",
    "    line_search_options : dict, LineSearchTool or None\n",
    "        Dictionary with line search options. See LineSearchTool class for details.\n",
    "    trace : bool\n",
    "        If True, the progress information is appended into history dictionary during training.\n",
    "        Otherwise None is returned instead of history.\n",
    "    display : bool\n",
    "        If True, debug information is displayed during optimization.\n",
    "        Printing format and is up to a student and is not checked in any way.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_star : np.array\n",
    "        The point found by the optimization procedure\n",
    "    message : string\n",
    "        \"success\" or the description of error:\n",
    "            - 'iterations_exceeded': if after max_iter iterations of the method x_k still doesn't satisfy\n",
    "                the stopping criterion.\n",
    "            - 'computational_error': in case of getting Infinity or None value during the computations.\n",
    "    history : dictionary of lists or None\n",
    "        Dictionary containing the progress information or None if trace=False.\n",
    "        Dictionary has to be organized as follows:\n",
    "            - history['time'] : list of floats, containing time in seconds passed from the start of the method\n",
    "            - history['func'] : list of function values f(x_k) on every step of the algorithm\n",
    "            - history['grad_norm'] : list of values Euclidian norms ||g(x_k)|| of the gradient on every step of the algorithm\n",
    "            - history['x'] : list of np.arrays, containing the trajectory of the algorithm. ONLY STORE IF x.size <= 2\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >> oracle = QuadraticOracle(np.eye(5), np.arange(5))\n",
    "    >> x_opt, message, history = gradient_descent(oracle, np.zeros(5), line_search_options={'method': 'Armijo', 'c1': 1e-4})\n",
    "    >> print('Found optimal point: {}'.format(x_opt))\n",
    "       Found optimal point: [ 0.  1.  2.  3.  4.]\n",
    "    \"\"\"\n",
    "    history = defaultdict(list) if trace else None\n",
    "    line_search_tool = get_line_search_tool(line_search_options)\n",
    "    x_k = np.copy(x_0)\n",
    "    \n",
    "    alpha_k = None\n",
    "    stop = tolerance * (np.linalg.norm(oracle.grad(x_0)) ** 2)\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for it in range(max_iter + 1):\n",
    "        grad_k = oracle.grad(x_k)\n",
    "        alpha_k = line_search_tool.line_search(oracle, x_k, -grad_k, 2 * alpha_k if alpha_k else None)\n",
    "        check = display_and_check(\n",
    "            it, x_k, np.linalg.norm(grad_k), stop, oracle, history, display, trace, start_time)\n",
    "        if check:\n",
    "            return check\n",
    "        x_k = x_k - grad_k * alpha_k\n",
    "    return x_k, 'iterations_exceeded', history\n",
    "\n",
    "\n",
    "def newton(oracle, x_0, tolerance=1e-5, max_iter=100,\n",
    "           line_search_options=None, trace=False, display=False):\n",
    "    \"\"\"\n",
    "    Newton's optimization method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    oracle : BaseSmoothOracle-descendant object\n",
    "        Oracle with .func(), .grad() and .hess() methods implemented for computing\n",
    "        function value, its gradient and Hessian respectively. If the Hessian\n",
    "        returned by the oracle is not positive-definite method stops with message=\"newton_direction_error\"\n",
    "    x_0 : np.array\n",
    "        Starting point for optimization algorithm\n",
    "    tolerance : float\n",
    "        Epsilon value for stopping criterion.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations.\n",
    "    line_search_options : dict, LineSearchTool or None\n",
    "        Dictionary with line search options. See LineSearchTool class for details.\n",
    "    trace : bool\n",
    "        If True, the progress information is appended into history dictionary during training.\n",
    "        Otherwise None is returned instead of history.\n",
    "    display : bool\n",
    "        If True, debug information is displayed during optimization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_star : np.array\n",
    "        The point found by the optimization procedure\n",
    "    message : string\n",
    "        'success' or the description of error:\n",
    "            - 'iterations_exceeded': if after max_iter iterations of the method x_k still doesn't satisfy\n",
    "                the stopping criterion.\n",
    "            - 'newton_direction_error': in case of failure of solving linear system with Hessian matrix (e.g. non-invertible matrix).\n",
    "            - 'computational_error': in case of getting Infinity or None value during the computations.\n",
    "    history : dictionary of lists or None\n",
    "        Dictionary containing the progress information or None if trace=False.\n",
    "        Dictionary has to be organized as follows:\n",
    "            - history['time'] : list of floats, containing time passed from the start of the method\n",
    "            - history['func'] : list of function values f(x_k) on every step of the algorithm\n",
    "            - history['grad_norm'] : list of values Euclidian norms ||g(x_k)|| of the gradient on every step of the algorithm\n",
    "            - history['x'] : list of np.arrays, containing the trajectory of the algorithm. ONLY STORE IF x.size <= 2\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >> oracle = QuadraticOracle(np.eye(5), np.arange(5))\n",
    "    >> x_opt, message, history = newton(oracle, np.zeros(5), line_search_options={'method': 'Constant', 'c': 1.0})\n",
    "    >> print('Found optimal point: {}'.format(x_opt))\n",
    "       Found optimal point: [ 0.  1.  2.  3.  4.]\n",
    "    \"\"\"\n",
    "    history = defaultdict(list) if trace else None\n",
    "    line_search_tool = get_line_search_tool(line_search_options)\n",
    "    x_k, norm0 = np.copy(x_0), np.linalg.norm(oracle.grad(x_0)) ** 2\n",
    "    stop = tolerance * norm0\n",
    "    start = datetime.now()\n",
    "\n",
    "    for it in range(max_iter + 1):\n",
    "        grad_k = oracle.grad(x_k)\n",
    "        check = display_and_check(it, x_k, np.linalg.norm(grad_k), stop, oracle, history, display, trace, start)\n",
    "        if check:\n",
    "            return check\n",
    "        try:\n",
    "            hess = oracle.hess(x_k)\n",
    "            d_k = scipy.linalg.cho_solve(scipy.linalg.cho_factor(hess), -grad_k)\n",
    "        except LinAlgError:\n",
    "            return x_k, 'computational_error', history\n",
    "        a_k = line_search_tool.line_search(oracle, x_k, d_k)\n",
    "        x_k = x_k + d_k * a_k\n",
    "    \n",
    "    return x_k, 'iterations_exceeded', history\n",
    "\n",
    "def displaying(iteration, x, norm):\n",
    "    print(\"iteration {0:4}:\".format(iteration))\n",
    "    print(\"x = {0}\".format(x))\n",
    "    print(\"norm = {0}\".format(norm))\n",
    "\n",
    "def fit_history(history, x, start, func, norm):\n",
    "    if x.size <= 2:\n",
    "        history['x'].append(x)\n",
    "    history['time'].append(datetime.now() - start)\n",
    "    history['func'].append(func)\n",
    "    history['grad_norm'].append(norm)\n",
    "\n",
    "def display_and_check(it, x, norm, stop, oracle, history, display, trace, start):\n",
    "    if display:\n",
    "        displaying(it, x, norm)\n",
    "    if trace:\n",
    "        fit_history(history, x, start, oracle.func(x), norm)\n",
    "    if norm ** 2 <= stop:\n",
    "        return x, 'success', history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c979a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class BaseSmoothOracle(object):\n",
    "    \"\"\"\n",
    "    Base class for implementation of oracles.\n",
    "    \"\"\"\n",
    "    def func(self, x):\n",
    "        \"\"\"\n",
    "        Computes the value of function at point x.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Func oracle is not implemented.')\n",
    "\n",
    "    def grad(self, x):\n",
    "        \"\"\"\n",
    "        Computes the gradient at point x.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Grad oracle is not implemented.')\n",
    "    \n",
    "    def hess(self, x):\n",
    "        \"\"\"\n",
    "        Computes the Hessian matrix at point x.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Hessian oracle is not implemented.')\n",
    "    \n",
    "    def func_directional(self, x, d, alpha):\n",
    "        \"\"\"\n",
    "        Computes phi(alpha) = f(x + alpha*d).\n",
    "        \"\"\"\n",
    "        return np.squeeze(self.func(x + alpha * d))\n",
    "\n",
    "    def grad_directional(self, x, d, alpha):\n",
    "        \"\"\"\n",
    "        Computes phi'(alpha) = (f(x + alpha*d))'_{alpha}\n",
    "        \"\"\"\n",
    "        return np.squeeze(self.grad(x + alpha * d).dot(d))\n",
    "\n",
    "\n",
    "class QuadraticOracle(BaseSmoothOracle):\n",
    "    \"\"\"\n",
    "    Oracle for quadratic function:\n",
    "       func(x) = 1/2 x^TAx - b^Tx.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, A, b):\n",
    "        if not scipy.sparse.isspmatrix_dia(A) and not np.allclose(A, A.T):\n",
    "            raise ValueError('A should be a symmetric matrix.')\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "\n",
    "    def func(self, x):\n",
    "        return 0.5 * np.dot(self.A.dot(x), x) - self.b.dot(x)\n",
    "\n",
    "    def grad(self, x):\n",
    "        return self.A.dot(x) - self.b\n",
    "\n",
    "    def hess(self, x):\n",
    "        return self.A \n",
    "\n",
    "\n",
    "class LogRegL2Oracle(BaseSmoothOracle):\n",
    "    \"\"\"\n",
    "    Oracle for logistic regression with l2 regularization:\n",
    "         func(x) = 1/m sum_i log(1 + exp(-b_i * a_i^T x)) + regcoef / 2 ||x||_2^2.\n",
    "\n",
    "    Let A and b be parameters of the logistic regression (feature matrix\n",
    "    and labels vector respectively).\n",
    "    For user-friendly interface use create_log_reg_oracle()\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        matvec_Ax : function\n",
    "            Computes matrix-vector product Ax, where x is a vector of size n.\n",
    "        matvec_ATx : function of x\n",
    "            Computes matrix-vector product A^Tx, where x is a vector of size m.\n",
    "        matmat_ATsA : function\n",
    "            Computes matrix-matrix-matrix product A^T * Diag(s) * A,\n",
    "    \"\"\"\n",
    "    def __init__(self, matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef):\n",
    "        self.matvec_Ax = matvec_Ax\n",
    "        self.matvec_ATx = matvec_ATx\n",
    "        self.matmat_ATsA = matmat_ATsA\n",
    "        self.b = b\n",
    "        self.regcoef = regcoef\n",
    "\n",
    "    def func(self, x):\n",
    "        sum_log = np.logaddexp(0, - self.b * self.matvec_Ax(x))\n",
    "        return np.linalg.norm(sum_log, 1) / self.b.size + np.linalg.norm(x, 2) ** 2 * self.regcoef / 2\n",
    "\n",
    "    def grad(self, x):\n",
    "        return self.regcoef * x - self.matvec_ATx(self.b * (expit(-self.b * self.matvec_Ax(x)))) / self.b.size\n",
    "    \n",
    "    def hess(self, x):\n",
    "        tmp = expit(self.b * self.matvec_Ax(x))\n",
    "        return self.matmat_ATsA(tmp * (1 - tmp)) / self.b.size + self.regcoef * np.identity(x.size)\n",
    "\n",
    "class LogRegL2OptimizedOracle(LogRegL2Oracle):\n",
    "    \"\"\"\n",
    "    Oracle for logistic regression with l2 regularization\n",
    "    with optimized *_directional methods (are used in line_search).\n",
    "    For explanation see LogRegL2Oracle.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef):\n",
    "        super().__init__(matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef)\n",
    "        self.prev_x = None\n",
    "        self.prev_Ax = None\n",
    "        self.prev_new_x = None\n",
    "        self.prev_d = None\n",
    "        self.prev_Ad = None\n",
    "        self.prev_new_Ax = None\n",
    "\n",
    "    def check_Av(self, v, key=''):\n",
    "        x_to_res = dict((('prev_x', 'prev_Ax'), ('prev_d', 'prev_Ad'), ('prev_new_x', 'prev_new_Ax')))\n",
    "        for u, k in x_to_res.items():\n",
    "            tmp = getattr(self, u, None)\n",
    "            if tmp is not None and np.all(tmp == v):\n",
    "                return getattr(self, k)\n",
    "\n",
    "        if key:\n",
    "            Av = self.matvec_Ax(v)\n",
    "            setattr(self, x_to_res[key], Av)\n",
    "            setattr(self, key, v)\n",
    "            return Av\n",
    "\n",
    "    def func(self, x):\n",
    "        Ax = self.check_Av(x, 'prev_x')\n",
    "        return np.linalg.norm(np.logaddexp(0, -self.b * Ax), 1) / self.b.size + \\\n",
    "               (self.regcoef * np.linalg.norm(x, 2) ** 2) / 2\n",
    "\n",
    "    def grad(self, x):\n",
    "        Ax = self.check_Av(x, 'prev_x')\n",
    "        return self.regcoef * x - self.matvec_ATx(self.b * (expit(- self.b * Ax))) / self.b.size\n",
    "\n",
    "    def hess(self, x):\n",
    "        Ax = self.check_Av(x, 'prev_x')\n",
    "        tmp = expit(self.b * Ax)\n",
    "        return self.matmat_ATsA(tmp * (1 - tmp)) / self.b.size + self.regcoef * np.identity(x.size)\n",
    "\n",
    "    def func_directional(self, x, d, alpha):\n",
    "        new_x = x + alpha * d\n",
    "\n",
    "        new_Ax = self.check_Av(new_x)\n",
    "        if new_Ax is None:\n",
    "            Ax = self.check_Av(x, 'prev_x')\n",
    "            Ad = self.check_Av(d, 'prev_d')\n",
    "            new_Ax = Ax + alpha * Ad\n",
    "            self.prev_new_x = new_x\n",
    "            self.prev_new_Ax = new_Ax\n",
    "\n",
    "        return np.linalg.norm(np.logaddexp(0, -self.b * new_Ax), 1) / self.b.size + \\\n",
    "               (self.regcoef * np.linalg.norm(new_x, 2) ** 2) / 2\n",
    "\n",
    "    def grad_directional(self, x, d, alpha):\n",
    "        new_x = x + alpha * d\n",
    "\n",
    "        new_Ax = self.check_Av(new_x)\n",
    "        Ad = self.check_Av(d, 'prev_d')\n",
    "        if new_Ax is None:\n",
    "            Ax = self.check_Av(x, 'prev_x')\n",
    "            new_Ax = Ax + alpha * Ad\n",
    "            self.prev_new_x = new_x\n",
    "            self.prev_new_Ax = new_Ax\n",
    "\n",
    "        return self.regcoef * np.dot(np.transpose(new_x), d) - \\\n",
    "               np.dot(self.b * (expit(- self.b * new_Ax)), Ad) / self.b.size\n",
    "\n",
    "\n",
    "def create_log_reg_oracle(A, b, regcoef, oracle_type='usual'):\n",
    "    \"\"\"\n",
    "    Auxiliary function for creating logistic regression oracles.\n",
    "        `oracle_type` must be either 'usual' or 'optimized'\n",
    "    \"\"\"\n",
    "    matvec_Ax = lambda x: A.dot(x)\n",
    "    matvec_ATx = lambda x: A.T.dot(x)\n",
    "\n",
    "    if scipy.sparse.issparse(A):\n",
    "        A = scipy.sparse.csr_matrix(A)\n",
    "        matvec_Ax = lambda x: A.dot(x)\n",
    "        matvec_ATx = lambda x: A.T.dot(x)\n",
    "        matmat_ATsA = lambda x: matvec_ATx(matvec_ATx(scipy.sparse.diags(x)).T)\n",
    "    else:\n",
    "        matmat_ATsA = lambda x: np.dot(matvec_ATx(np.diag(x)), A)\n",
    "    \n",
    "    if oracle_type == 'usual':\n",
    "        oracle = LogRegL2Oracle\n",
    "    elif oracle_type == 'optimized':\n",
    "        oracle = LogRegL2OptimizedOracle\n",
    "    else:\n",
    "        raise 'Unknown oracle type=%s' % oracle_type\n",
    "    return oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef)\n",
    "\n",
    "\n",
    "def grad_finite_diff(func, x, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Returns approximation of the gradient using finite differences:\n",
    "        result_i := (f(x + eps * e_i) - f(x)) / eps,\n",
    "        where e_i are coordinate vectors:\n",
    "        e_i = (0, 0, ..., 0, 1, 0, ..., 0)\n",
    "                          >> i <<\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    f_x = func(x)\n",
    "    e = np.identity(x.size)\n",
    "    res = np.zeros_like(x)\n",
    "\n",
    "    for i in range(x.size):\n",
    "        res[i] = (func(x + e[i] * eps) - f_x) / eps\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def hess_finite_diff(func, x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Returns approximation of the Hessian using finite differences:\n",
    "        result_{ij} := (f(x + eps * e_i + eps * e_j)\n",
    "                               - f(x + eps * e_i)\n",
    "                               - f(x + eps * e_j)\n",
    "                               + f(x)) / eps^2,\n",
    "        where e_i are coordinate vectors:\n",
    "        e_i = (0, 0, ..., 0, 1, 0, ..., 0)\n",
    "                          >> i <<\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    f_x = func(x)\n",
    "    res, tmp, e = np.zeros((x.size, x.size)), np.zeros_like(x), np.identity(x.size)\n",
    "\n",
    "    for i in range(x.size):\n",
    "        tmp[i] = func(x + e[i] * eps)\n",
    "\n",
    "    for i in range(x.size):\n",
    "        for j in range(x.size):\n",
    "            res[i][j] = (func(x + eps * (e[i] + e[j]))- tmp[j] - tmp[i] + f_x) / (eps ** 2)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0bffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nose\n",
    "from nose.tools import assert_almost_equal, ok_, eq_\n",
    "from nose.plugins.attrib import attr\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import scipy.optimize\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "def test_python3():\n",
    "    ok_(sys.version_info > (3, 0))\n",
    "\n",
    "\n",
    "def test_QuadraticOracle():\n",
    "    # Quadratic function:\n",
    "    #   f(x) = 1/2 x^T x - [1, 2, 3]^T x\n",
    "    A = np.eye(3)\n",
    "    b = np.array([1, 2, 3])\n",
    "    quadratic = QuadraticOracle(A, b)\n",
    "\n",
    "    # Check at point x = [0, 0, 0]\n",
    "    x = np.zeros(3)\n",
    "    assert_almost_equal(quadratic.func(x), 0.0)\n",
    "    ok_(np.allclose(quadratic.grad(x), -b))\n",
    "    ok_(np.allclose(quadratic.hess(x), A))\n",
    "    ok_(isinstance(quadratic.grad(x), np.ndarray))\n",
    "    ok_(isinstance(quadratic.hess(x), np.ndarray))\n",
    "\n",
    "    # Check at point x = [1, 1, 1]\n",
    "    x = np.ones(3)\n",
    "    assert_almost_equal(quadratic.func(x), -4.5)\n",
    "    ok_(np.allclose(quadratic.grad(x), x - b))\n",
    "    ok_(np.allclose(quadratic.hess(x), A))\n",
    "    ok_(isinstance(quadratic.grad(x), np.ndarray))\n",
    "    ok_(isinstance(quadratic.hess(x), np.ndarray))\n",
    "\n",
    "    # Check func_direction and grad_direction oracles at\n",
    "    # x = [1, 1, 1], d = [-1, -1, -1], alpha = 0.5 and 1.0\n",
    "    x = np.ones(3)\n",
    "    d = -np.ones(3)\n",
    "    assert_almost_equal(quadratic.func_directional(x, d, alpha=0.5),\n",
    "                        -2.625)\n",
    "    assert_almost_equal(quadratic.grad_directional(x, d, alpha=0.5),\n",
    "                        4.5)\n",
    "    assert_almost_equal(quadratic.func_directional(x, d, alpha=1.0),\n",
    "                        0.0)\n",
    "    assert_almost_equal(quadratic.grad_directional(x, d, alpha=1.0),\n",
    "                        6.0)\n",
    "\n",
    "\n",
    "def check_log_reg(oracle_type, sparse=False):\n",
    "    # Simple data:\n",
    "    A = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    if sparse: A = scipy.sparse.csr_matrix(A)\n",
    "    b = np.array([1, 1, -1, 1])\n",
    "    reg_coef = 0.5\n",
    "\n",
    "    # Logistic regression oracle:\n",
    "    logreg = create_log_reg_oracle(A, b, reg_coef, oracle_type=oracle_type)\n",
    "\n",
    "    # Check at point x = [0, 0]\n",
    "    x = np.zeros(2)\n",
    "    assert_almost_equal(logreg.func(x), 0.693147180)\n",
    "    ok_(np.allclose(logreg.grad(x), [0, -0.25]))\n",
    "    ok_(np.allclose(logreg.hess(x), [[0.625, 0.0625], [0.0625, 0.625]]))\n",
    "    ok_(isinstance(logreg.grad(x), np.ndarray))\n",
    "    ok_(isinstance(logreg.hess(x), np.ndarray))\n",
    "\n",
    "    # Check func_direction and grad_direction oracles at\n",
    "    # x = [0, 0], d = [1, 1], alpha = 0.5 and 1.0\n",
    "    x = np.zeros(2)\n",
    "    d = np.ones(2)\n",
    "    assert_almost_equal(logreg.func_directional(x, d, alpha=0.5),\n",
    "                        0.7386407091095)\n",
    "    assert_almost_equal(logreg.grad_directional(x, d, alpha=0.5),\n",
    "                        0.4267589549159)\n",
    "    assert_almost_equal(logreg.func_directional(x, d, alpha=1.0),\n",
    "                        1.1116496416598)\n",
    "    assert_almost_equal(logreg.grad_directional(x, d, alpha=1.0),\n",
    "                        1.0559278283039)\n",
    "\n",
    "\n",
    "def test_log_reg_usual():\n",
    "    check_log_reg('usual')\n",
    "    check_log_reg('usual', sparse=True)\n",
    "\n",
    "\n",
    "@attr('bonus')\n",
    "def test_log_reg_optimized():\n",
    "    check_log_reg('optimized')\n",
    "    check_log_reg('optimized', sparse=True)\n",
    "\n",
    "\n",
    "def get_counters(A):\n",
    "    counters = {'Ax': 0, 'ATx': 0, 'ATsA': 0}\n",
    "\n",
    "    def matvec_Ax(x):\n",
    "        counters['Ax'] += 1\n",
    "        return A.dot(x)\n",
    "\n",
    "    def matvec_ATx(x):\n",
    "        counters['ATx'] += 1\n",
    "        return A.T.dot(x)\n",
    "\n",
    "    def matmat_ATsA(s):\n",
    "        counters['ATsA'] += 1\n",
    "        return A.T.dot(A * s.reshape(-1, 1))\n",
    "\n",
    "    return (matvec_Ax, matvec_ATx, matmat_ATsA, counters)\n",
    "\n",
    "\n",
    "def check_counters(counters, groundtruth):\n",
    "    for (key, value) in groundtruth.items():\n",
    "        ok_(key in counters)\n",
    "        ok_(counters[key] <= value)\n",
    "\n",
    "\n",
    "def test_log_reg_oracle_calls():\n",
    "\n",
    "    A = np.ones((2, 2))\n",
    "    b = np.ones(2)\n",
    "    x = np.ones(2)\n",
    "    d = np.ones(2)\n",
    "    reg_coef = 0.5\n",
    "\n",
    "    # Single func\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    LogRegL2Oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef).func(x)\n",
    "    check_counters(counters, {'Ax': 1, 'ATx': 0, 'ATsA': 0})\n",
    "\n",
    "    # Single grad\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    LogRegL2Oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef).grad(x)\n",
    "    check_counters(counters, {'Ax': 1, 'ATx': 1, 'ATsA': 0})\n",
    "\n",
    "    # Single hess\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    LogRegL2Oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef).hess(x)\n",
    "    check_counters(counters, {'Ax': 1, 'ATx': 0, 'ATsA': 1})\n",
    "\n",
    "    # Single func_directional\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    LogRegL2Oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef).func_directional(x, d, 1)\n",
    "    check_counters(counters, {'Ax': 1, 'ATx': 0, 'ATsA': 0})\n",
    "\n",
    "    # Single grad_directional\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    LogRegL2Oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef).grad_directional(x, d, 1)\n",
    "    check_counters(counters, {'Ax': 1, 'ATx': 1, 'ATsA': 0})\n",
    "\n",
    "    # In a row: func + grad + hess\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    oracle = LogRegL2Oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef)\n",
    "    oracle.func(x)\n",
    "    oracle.grad(x)\n",
    "    oracle.hess(x)\n",
    "    check_counters(counters, {'Ax': 3, 'ATx': 1, 'ATsA': 1})\n",
    "\n",
    "    # In a row: func + grad\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    oracle = LogRegL2Oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef)\n",
    "    oracle.func(x)\n",
    "    oracle.grad(x)\n",
    "    check_counters(counters, {'Ax': 2, 'ATx': 1, 'ATsA': 0})\n",
    "\n",
    "    # In a row: grad + hess\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    oracle = LogRegL2Oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef)\n",
    "    oracle.grad(x)\n",
    "    oracle.hess(x)\n",
    "    check_counters(counters, {'Ax': 2, 'ATx': 1, 'ATsA': 1})\n",
    "\n",
    "    # In a row: func + grad + func_directional + grad_directional\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    oracle = LogRegL2Oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef)\n",
    "    oracle.func(x)\n",
    "    oracle.grad(x)\n",
    "    oracle.func_directional(x, d, 1)\n",
    "    oracle.grad_directional(x, d, 2)\n",
    "    oracle.func_directional(x, d, 2)\n",
    "    oracle.func_directional(x, d, 3)\n",
    "    check_counters(counters, {'Ax': 6, 'ATx': 2, 'ATsA': 0})\n",
    "\n",
    "    # In a row: func + grad + func_directional + grad_directional + (func + grad)\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    oracle = LogRegL2Oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef)\n",
    "    oracle.func(x)\n",
    "    oracle.grad(x)\n",
    "    oracle.func_directional(x, d, 1)\n",
    "    oracle.grad_directional(x, d, 2)\n",
    "    oracle.func_directional(x, d, 2)\n",
    "    oracle.func_directional(x, d, 3)\n",
    "    oracle.func(x + 3 * d)\n",
    "    oracle.grad(x + 3 * d)\n",
    "    check_counters(counters, {'Ax': 8, 'ATx': 3, 'ATsA': 0})\n",
    "\n",
    "\n",
    "@attr('bonus')\n",
    "def test_log_reg_optimized_oracle_calls():\n",
    "\n",
    "    A = np.ones((2, 2))\n",
    "    b = np.ones(2)\n",
    "    x = np.ones(2)\n",
    "    d = np.ones(2)\n",
    "    reg_coef = 0.5\n",
    "\n",
    "    # Single func\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    LogRegL2OptimizedOracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef).func(x)\n",
    "    check_counters(counters, {'Ax': 1, 'ATx': 0, 'ATsA': 0})\n",
    "\n",
    "    # Single grad\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    LogRegL2OptimizedOracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef).grad(x)\n",
    "    check_counters(counters, {'Ax': 1, 'ATx': 1, 'ATsA': 0})\n",
    "\n",
    "    # Single hess\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    LogRegL2OptimizedOracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef).hess(x)\n",
    "    check_counters(counters, {'Ax': 1, 'ATx': 0, 'ATsA': 1})\n",
    "\n",
    "    # Single func_directional\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    LogRegL2OptimizedOracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef).func_directional(x, d, 1)\n",
    "    check_counters(counters, {'Ax': 2, 'ATx': 0, 'ATsA': 0})\n",
    "\n",
    "    # Single grad_directional\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    LogRegL2OptimizedOracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef).grad_directional(x, d, 1)\n",
    "    check_counters(counters, {'Ax': 2, 'ATx': 0, 'ATsA': 0})\n",
    "\n",
    "    # In a row: func + grad + hess\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    oracle = LogRegL2OptimizedOracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef)\n",
    "    oracle.func(x)\n",
    "    oracle.grad(x)\n",
    "    oracle.hess(x)\n",
    "    check_counters(counters, {'Ax': 1, 'ATx': 1, 'ATsA': 1})\n",
    "\n",
    "    # In a row: func + grad\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    oracle = LogRegL2OptimizedOracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef)\n",
    "    oracle.func(x)\n",
    "    oracle.grad(x)\n",
    "    check_counters(counters, {'Ax': 1, 'ATx': 1, 'ATsA': 0})\n",
    "\n",
    "    # In a row: grad + hess\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    oracle = LogRegL2OptimizedOracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef)\n",
    "    oracle.grad(x)\n",
    "    oracle.hess(x)\n",
    "    check_counters(counters, {'Ax': 1, 'ATx': 1, 'ATsA': 1})\n",
    "\n",
    "    # In a row: func + grad + func_directional + grad_directional\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    oracle = LogRegL2OptimizedOracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef)\n",
    "    oracle.func(x)\n",
    "    oracle.grad(x)\n",
    "    oracle.func_directional(x, d, 1)\n",
    "    oracle.grad_directional(x, d, 2)\n",
    "    oracle.func_directional(x, d, 2)\n",
    "    oracle.func_directional(x, d, 3)\n",
    "    check_counters(counters, {'Ax': 2, 'ATx': 1, 'ATsA': 0})\n",
    "\n",
    "    # In a row: func + grad + func_directional + grad_directional + (func + grad)\n",
    "    (matvec_Ax, matvec_ATx, matmat_ATsA, counters) = get_counters(A)\n",
    "    oracle = LogRegL2OptimizedOracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, reg_coef)\n",
    "    oracle.func(x)\n",
    "    oracle.grad(x)\n",
    "    oracle.func_directional(x, d, 1)\n",
    "    oracle.grad_directional(x, d, 2)\n",
    "    oracle.func_directional(x, d, 2)\n",
    "    oracle.func_directional(x, d, 3)\n",
    "    oracle.func(x + 3 * d)\n",
    "    oracle.grad(x + 3 * d)\n",
    "    check_counters(counters, {'Ax': 2, 'ATx': 2, 'ATsA': 0})\n",
    "\n",
    "\n",
    "def test_grad_finite_diff_1():\n",
    "    # Quadratic function.\n",
    "    A = np.eye(3)\n",
    "    b = np.array([1, 2, 3])\n",
    "    quadratic = QuadraticOracle(A, b)\n",
    "    g = grad_finite_diff(quadratic.func, np.zeros(3))\n",
    "    ok_(isinstance(g, np.ndarray))\n",
    "    ok_(np.allclose(g, -b))\n",
    "\n",
    "\n",
    "def test_grad_finite_diff_2():\n",
    "    # f(x, y) = x^3 + y^2\n",
    "    func = lambda x: x[0] ** 3 + x[1] ** 2\n",
    "    x = np.array([2.0, 3.0])\n",
    "    eps = 1e-5\n",
    "    g = grad_finite_diff(func, x, eps)\n",
    "    ok_(isinstance(g, np.ndarray))\n",
    "    ok_(np.allclose(g, [12.0, 6.0], atol=1e-4))\n",
    "\n",
    "\n",
    "def test_hess_finite_diff_1():\n",
    "    # Quadratic function.\n",
    "    A = np.eye(3)\n",
    "    b = np.array([1, 2, 3])\n",
    "    quadratic = QuadraticOracle(A, b)\n",
    "    H = hess_finite_diff(quadratic.func, np.zeros(3))\n",
    "    ok_(isinstance(H, np.ndarray))\n",
    "    ok_(np.allclose(H, A))\n",
    "\n",
    "\n",
    "def test_hess_finite_diff_2():\n",
    "    # f(x, y) = x^3 + y^2\n",
    "    func = lambda x: x[0] ** 3 + x[1] ** 2\n",
    "    x = np.array([2.0, 3.0])\n",
    "    eps = 1e-5\n",
    "    H = hess_finite_diff(func, x, eps)\n",
    "    ok_(isinstance(H, np.ndarray))\n",
    "    ok_(np.allclose(H, [[12.0, 0.], [0., 2.0]], atol=1e-3))\n",
    "\n",
    "\n",
    "def get_quadratic():\n",
    "    # Quadratic function:\n",
    "    #   f(x) = 1/2 x^T x - [1, 2, 3]^T x\n",
    "    A = np.eye(3)\n",
    "    b = np.array([1, 2, 3])\n",
    "    return QuadraticOracle(A, b)\n",
    "\n",
    "\n",
    "def test_line_search():\n",
    "    oracle = get_quadratic()\n",
    "    x = np.array([100, 0, 0])\n",
    "    d = np.array([-1, 0, 0])\n",
    "\n",
    "    # Constant line search\n",
    "    ls_tool = LineSearchTool(method='Constant', c=1.0)\n",
    "    assert_almost_equal(ls_tool.line_search(oracle, x, d, ), 1.0)\n",
    "    ls_tool = LineSearchTool(method='Constant', c=10.0)\n",
    "    assert_almost_equal(ls_tool.line_search(oracle, x, d), 10.0)\n",
    "\n",
    "    # Armijo rule\n",
    "    ls_tool = LineSearchTool(method='Armijo', alpha_0=100, c1=0.9)\n",
    "    assert_almost_equal(ls_tool.line_search(oracle, x, d), 12.5)\n",
    "\n",
    "    ls_tool = LineSearchTool(method='Armijo', alpha_0=100, c1=0.9)\n",
    "    assert_almost_equal(ls_tool.line_search(oracle, x, d, previous_alpha=1.0), 1.0)\n",
    "\n",
    "    ls_tool = LineSearchTool(method='Armijo', alpha_0=100, c1=0.95)\n",
    "    assert_almost_equal(ls_tool.line_search(oracle, x, d), 6.25)\n",
    "    ls_tool = LineSearchTool(method='Armijo', alpha_0=10, c1=0.9)\n",
    "    assert_almost_equal(ls_tool.line_search(oracle, x, d), 10.0)\n",
    "\n",
    "    # Wolfe rule\n",
    "    ls_tool = LineSearchTool(method='Wolfe', c1=1e-4, c2=0.9)\n",
    "    assert_almost_equal(ls_tool.line_search(oracle, x, d), 16.0)\n",
    "    ls_tool = LineSearchTool(method='Wolfe', c1=1e-4, c2=0.8)\n",
    "    assert_almost_equal(ls_tool.line_search(oracle, x, d), 32.0)\n",
    "\n",
    "\n",
    "def check_equal_histories(history1, history2, atol=1e-3):\n",
    "    if history1 is None or history2 is None:\n",
    "        eq_(history1, history2)\n",
    "        return\n",
    "    ok_('func' in history1 and 'func' in history2)\n",
    "    ok_(np.allclose(history1['func'], history2['func'], atol=atol))\n",
    "    ok_('grad_norm' in history1 and 'grad_norm' in history2)\n",
    "    ok_(np.allclose(history1['grad_norm'], history2['grad_norm'], atol=atol))\n",
    "    ok_('time' in history1 and 'time' in history2)\n",
    "    eq_(len(history1['time']), len(history2['time']))\n",
    "    eq_('x' in history1, 'x' in history2)\n",
    "    if 'x' in history1:\n",
    "        ok_(np.allclose(history1['x'], history2['x'], atol=atol))\n",
    "\n",
    "\n",
    "def check_prototype(method):\n",
    "    class ZeroOracle2D(BaseSmoothOracle):\n",
    "        def func(self, x): return 0.0\n",
    "\n",
    "        def grad(self, x): return np.zeros(2)\n",
    "\n",
    "        def hess(self, x): return np.zeros([2, 2])\n",
    "\n",
    "    oracle = ZeroOracle2D()\n",
    "    x0 = np.ones(2)\n",
    "    HISTORY = {'func': [0.0],\n",
    "               'grad_norm': [0.0],\n",
    "               'time': [0],  # dummy timestamp\n",
    "               'x': [np.ones(2)]}\n",
    "\n",
    "    def check_result(result, x0=np.ones(2), msg='success', history=None):\n",
    "        eq_(len(result), 3)\n",
    "        ok_(np.allclose(result[0], x0))\n",
    "        eq_(result[1], msg)\n",
    "        check_equal_histories(result[2], history)\n",
    "\n",
    "    check_result(method(oracle, x0))\n",
    "    check_result(method(oracle, x0, 1e-3, 10))\n",
    "    check_result(method(oracle, x0, 1e-3, 10, {'method': 'Constant', 'c': 1.0}))\n",
    "    check_result(method(oracle, x0, 1e-3, 10, {'method': 'Constant', 'c': 1.0},\n",
    "                        trace=True), history=HISTORY)\n",
    "    check_result(method(oracle, x0, 1e-3, max_iter=10,\n",
    "                        line_search_options={'method': 'Constant', 'c': 1.0},\n",
    "                        trace=True, display=True), history=HISTORY)\n",
    "    check_result(method(oracle, x0, display=True, trace=False))\n",
    "    check_result(method(oracle, x0, tolerance=1e-8, trace=True),\n",
    "                 history=HISTORY)\n",
    "\n",
    "    # Check default display=False\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = mystdout = StringIO()\n",
    "    check_result(method(oracle, x0))\n",
    "    eq_(mystdout.getvalue(), \"\")\n",
    "    sys.stdout = old_stdout\n",
    "    # Check specified display=False\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = mystdout = StringIO()\n",
    "    check_result(method(oracle, x0, display=False))\n",
    "    eq_(mystdout.getvalue(), \"\")\n",
    "    sys.stdout = old_stdout\n",
    "    # Check specified display=True\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = mystdout = StringIO()\n",
    "    check_result(method(oracle, x0, display=True))\n",
    "    ok_(len(mystdout.getvalue()) > 1)\n",
    "    sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "def check_one_ideal_step(method):\n",
    "    oracle = get_quadratic()\n",
    "    x0 = np.ones(3) * 10.0\n",
    "    [x_star, msg, history] = method(oracle, x0, max_iter=1,\n",
    "                                    tolerance=1e-5, trace=True)\n",
    "    ok_(np.allclose(x_star, [1.0, 2.0, 3.0]))\n",
    "    eq_(msg, 'success')\n",
    "    check_equal_histories(history, {'func': [90.0, -7.0],\n",
    "                                    'grad_norm': [13.928388277184119, 0.0],\n",
    "                                    'time': [0, 1]  # dummy timestamps\n",
    "                                    })\n",
    "\n",
    "\n",
    "def test_gd_basic():\n",
    "    check_prototype(gradient_descent)\n",
    "    check_one_ideal_step(gradient_descent)\n",
    "\n",
    "\n",
    "def test_newton_basic():\n",
    "    check_prototype(newton)\n",
    "    check_one_ideal_step(newton)\n",
    "\n",
    "\n",
    "def get_1d(alpha):\n",
    "    # 1D function:\n",
    "    #   f(x) = exp(alpha * x) + alpha * x^2\n",
    "    class Func(BaseSmoothOracle):\n",
    "        def __init__(self, alpha):\n",
    "            self.alpha = alpha\n",
    "\n",
    "        def func(self, x):\n",
    "            return np.exp(self.alpha * x) + self.alpha * x ** 2\n",
    "\n",
    "        def grad(self, x):\n",
    "            return np.array(self.alpha * np.exp(self.alpha * x) +\n",
    "                            2 * self.alpha * x)\n",
    "\n",
    "        def hess(self, x):\n",
    "            return np.array([self.alpha ** 2 * np.exp(self.alpha * x) +\n",
    "                             2 * self.alpha])\n",
    "\n",
    "    return Func(alpha)\n",
    "\n",
    "\n",
    "def test_gd_1d():\n",
    "    oracle = get_1d(0.5)\n",
    "    x0 = np.array([1.0])\n",
    "    FUNC = [\n",
    "        np.array([2.14872127]),\n",
    "        np.array([0.8988787]),\n",
    "        np.array([0.89869501]),\n",
    "        np.array([0.89869434]),\n",
    "        np.array([0.89869434])]\n",
    "    GRAD_NORM = [\n",
    "        1.8243606353500641,\n",
    "        0.021058536428132546,\n",
    "        0.0012677045924299746,\n",
    "        7.5436847232768223e-05,\n",
    "        4.485842052370792e-06]\n",
    "    TIME = [0] * 5  # Dummy values.\n",
    "    X = [\n",
    "        np.array([1.]),\n",
    "        np.array([-0.42528175]),\n",
    "        np.array([-0.40882976]),\n",
    "        np.array([-0.40783937]),\n",
    "        np.array([-0.40778044])]\n",
    "    TRUE_HISTORY = {'func': FUNC,\n",
    "                    'grad_norm': GRAD_NORM,\n",
    "                    'time': TIME,\n",
    "                    'x': X}\n",
    "    # Armijo rule.\n",
    "    [x_star, msg, history] = gradient_descent(\n",
    "        oracle, x0,\n",
    "        max_iter=5,\n",
    "        tolerance=1e-10,\n",
    "        trace=True,\n",
    "        line_search_options={\n",
    "            'method': 'Armijo',\n",
    "            'alpha_0': 100,\n",
    "            'c1': 0.3\n",
    "        }\n",
    "    )\n",
    "    ok_(np.allclose(x_star, [-0.4077], atol=1e-3))\n",
    "    eq_(msg, 'success')\n",
    "    check_equal_histories(history, TRUE_HISTORY)\n",
    "    # Constant step size.\n",
    "    [x_star, msg, history] = gradient_descent(oracle, x0,\n",
    "                                                           max_iter=5, tolerance=1e-10, trace=False,\n",
    "                                                           line_search_options={\n",
    "                                                               'method': 'Constant',\n",
    "                                                               'c': 1.0})\n",
    "    ok_(np.allclose(x_star, [-0.4084371], atol=1e-2))\n",
    "    eq_(msg, 'iterations_exceeded')\n",
    "    eq_(history, None)\n",
    "\n",
    "\n",
    "def test_newton_1d():\n",
    "    oracle = get_1d(0.5)\n",
    "    x0 = np.array([1.0])\n",
    "    FUNC = [\n",
    "        np.array([2.14872127]),\n",
    "        np.array([0.9068072]),\n",
    "        np.array([0.89869455]),\n",
    "        np.array([0.89869434])]\n",
    "    GRAD_NORM = [\n",
    "        1.8243606353500641,\n",
    "        0.14023069594489929,\n",
    "        0.00070465169721295462,\n",
    "        1.7464279966628027e-08]\n",
    "    TIME = [0] * 4  # Dummy values.\n",
    "    X = [\n",
    "        np.array([1.]),\n",
    "        np.array([-0.29187513]),\n",
    "        np.array([-0.40719141]),\n",
    "        np.array([-0.40777669])]\n",
    "    TRUE_HISTORY = {'func': FUNC,\n",
    "                    'grad_norm': GRAD_NORM,\n",
    "                    'time': TIME,\n",
    "                    'x': X}\n",
    "    # Constant step size.\n",
    "    [x_star, msg, history] = newton(\n",
    "        oracle, x0,\n",
    "        max_iter=5, tolerance=1e-10, trace=True,\n",
    "        line_search_options={\n",
    "            'method': 'Constant',\n",
    "            'c': 1.0}\n",
    "    )\n",
    "    ok_(np.allclose(x_star, [-0.4077777], atol=1e-4))\n",
    "    eq_(msg, 'success')\n",
    "    check_equal_histories(history, TRUE_HISTORY)\n",
    "\n",
    "\n",
    "def test_newton_fail():\n",
    "    # f(x) = integral_{-infty}^x arctan(t) dt\n",
    "    class Oracle(BaseSmoothOracle):\n",
    "        def func(self, x):\n",
    "            return x * np.arctan(x) - 0.5 * np.log(np.power(x, 2) + 1)\n",
    "\n",
    "        def grad(self, x):\n",
    "            return np.arctan(x)\n",
    "\n",
    "        def hess(self, x):\n",
    "            return np.array([1 / (np.power(x, 2) + 1)])\n",
    "\n",
    "    x0 = np.array([10.0])\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    [x_star, msg, history] = newton(Oracle(), x0,\n",
    "                                                display=False, trace=False,\n",
    "                                                 line_search_options={'method': 'Constant', 'c': 1})\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    eq_(msg, 'computational_error')\n",
    "    eq_(history, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4e6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_python3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e1d771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_QuadraticOracle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcfffefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_reg_usual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44446d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_reg_optimized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8650f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_reg_oracle_calls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b908ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_reg_optimized_oracle_calls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30347774",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grad_finite_diff_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d3a5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grad_finite_diff_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b02cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hess_finite_diff_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce6fa151",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hess_finite_diff_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49ccc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_line_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cd02633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration    0:\n",
      "x = [1. 1.]\n",
      "norm = 0.0\n",
      "iteration    0:\n",
      "x = [1. 1.]\n",
      "norm = 0.0\n"
     ]
    }
   ],
   "source": [
    "test_gd_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd47d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration    0:\n",
      "x = [1. 1.]\n",
      "norm = 0.0\n",
      "iteration    0:\n",
      "x = [1. 1.]\n",
      "norm = 0.0\n"
     ]
    }
   ],
   "source": [
    "test_newton_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13bf3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gd_1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ed0b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_newton_1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71ac04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_newton_fail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90dc7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
